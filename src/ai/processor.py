"""
AI å‡¦ç†çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import hashlib
import time
from datetime import datetime, timedelta
from typing import Any

from src.ai.gemini_client import GeminiAPIError, GeminiClient
from src.ai.models import (
    AIModelConfig,
    AIProcessingResult,
    ProcessingCache,
    ProcessingRequest,
    ProcessingSettings,
    ProcessingStats,
)
from src.utils.lru_cache import MemoryOptimizedCache
from src.utils.memory_manager import get_memory_manager
from src.utils.mixins import LoggerMixin


class AIProcessor(LoggerMixin):
    """AI å‡¦ç†çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        model_config: AIModelConfig | None = None,
        settings: ProcessingSettings | None = None,
    ):
        """
        AI å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–

        Args:
            model_config: AI ãƒ¢ãƒ‡ãƒ«è¨­å®š
            settings: å‡¦ç†è¨­å®š
        """
        self.settings = settings or ProcessingSettings()
        self.model_config = model_config or AIModelConfig()
        self.gemini_client = GeminiClient(self.model_config)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†ï¼ˆ LRU Cache ã«å¤‰æ›´ï¼‰
        self._cache = MemoryOptimizedCache(
            max_size=getattr(self.settings, "max_cache_entries", 500),
            ttl_hours=self.settings.cache_duration_hours,
        )
        self.stats = ProcessingStats()
        self._processing_queue: list[ProcessingRequest] = []
        self._is_processing = False

        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«ç™»éŒ²
        memory_manager = get_memory_manager()
        memory_manager.register_component(self)

        self.logger.info(
            "AI Processor initialized",
            cache_duration=self.settings.cache_duration_hours,
            model=self.model_config.model_name,
            min_text_length=self.settings.min_text_length,
            max_text_length=self.settings.max_text_length,
            enable_summary=self.settings.enable_summary,
            enable_tags=self.settings.enable_tags,
            enable_categorization=self.settings.enable_categorization,
        )

    def _generate_content_hash(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ"""
        return hashlib.sha256(text.encode("utf-8")).hexdigest()[:16]

    def _is_text_processable(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒå‡¦ç†å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
        result = self._check_text_processability(text)
        return result["is_processable"]

    def _check_text_processability(self, text: str) -> dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒå‡¦ç†å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰"""
        text_length = len(text.strip())
        stripped_text = text.strip()

        # ğŸ”§ FORCE FIX: æœ€å°é•·ã‚’å¼·åˆ¶çš„ã« 3 ã«è¨­å®šã—ã¦è¨­å®šã®å•é¡Œã‚’å›é¿
        min_length = 3  # self.settings.min_text_length ã®ä»£ã‚ã‚Šã«ç›´æ¥æŒ‡å®š
        max_length = getattr(self.settings, "max_text_length", 8000)

        self.logger.debug(
            "Checking text processability",
            text_length=text_length,
            min_length=min_length,
            max_length=max_length,
            text_preview=stripped_text[:100] + "..."
            if len(stripped_text) > 100
            else stripped_text,
        )

        # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
        if not stripped_text:
            return {"is_processable": False, "reason": "empty text"}

        # é•·ã•ãƒã‚§ãƒƒã‚¯ - å¼·åˆ¶çš„ã« 3 ã‚’ä½¿ç”¨
        if text_length < min_length:
            self.logger.warning(
                "âŒ Text too short for processing",
                length=text_length,
                min_length=min_length,
            )
            return {
                "is_processable": False,
                "reason": f"too short ({text_length} < {min_length})",
            }

        if text_length > max_length:
            self.logger.warning(
                "Text too long for processing",
                length=text_length,
                max_length=max_length,
            )
            return {
                "is_processable": False,
                "reason": f"too long ({text_length} > {max_length})",
            }

        # ç„¡åŠ¹æ–‡å­—ãƒã‚§ãƒƒã‚¯ (Control characters ãªã©)
        if any(ord(c) < 32 and c not in "\t\n\r" for c in stripped_text):
            return {
                "is_processable": False,
                "reason": "contains invalid control characters",
            }

        return {"is_processable": True, "reason": "valid"}

    def _get_from_cache(self, content_hash: str) -> AIProcessingResult | None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ï¼ˆ LRU æœ€é©åŒ–ï¼‰"""
        cache_entry = self._cache.get(content_hash)

        if cache_entry is None:
            return None

        # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°
        cache_entry.access()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
        result = cache_entry.result
        result.cache_hit = True

        self.logger.debug(
            "Cache hit",
            content_hash=content_hash,
            access_count=cache_entry.access_count,
        )

        return result

    def _save_to_cache(self, content_hash: str, result: AIProcessingResult) -> None:
        """çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆ LRU æœ€é©åŒ–ï¼‰"""
        expires_at = datetime.now() + timedelta(
            hours=self.settings.cache_duration_hours
        )

        cache_entry = ProcessingCache(
            content_hash=content_hash,
            result=result,
            created_at=datetime.now(),
            expires_at=expires_at,
        )

        self._cache.put(content_hash, cache_entry)

        self.logger.debug(
            "Result cached",
            content_hash=content_hash,
            expires_at=expires_at.isoformat(),
            cache_size=self._cache.size(),
        )

    def _clean_expired_cache(self) -> int:
        """æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼ˆ LRU æœ€é©åŒ–ï¼‰"""
        # LRU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœŸé™åˆ‡ã‚Œã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        expired_count = self._cache.cleanup_expired()

        if expired_count > 0:
            self.logger.info(f"Cleaned {expired_count} expired cache entries")

        return expired_count

    async def cleanup_expired(self) -> int:
        """ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹æ¸…ç†ãƒ¡ã‚½ãƒƒãƒ‰"""
        return self._clean_expired_cache()

    async def process_text(
        self, text: str, message_id: int, force_reprocess: bool = False
    ) -> AIProcessingResult:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã® AI å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            text: å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            message_id: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ID
            force_reprocess: å¼·åˆ¶å†å‡¦ç†ãƒ•ãƒ©ã‚°

        Returns:
            AI å‡¦ç†çµæœ
        """
        start_time = time.time()

        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        cleaned_text = text.strip()
        content_hash = self._generate_content_hash(cleaned_text)

        # å‡¦ç†å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ - ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        processable_result = self._check_text_processability(cleaned_text)
        if not processable_result["is_processable"]:
            self.logger.warning(
                "Text not processable",
                message_id=message_id,
                reason=processable_result["reason"],
                text_length=len(cleaned_text),
            )
            return AIProcessingResult(
                message_id=message_id,
                processed_at=datetime.now(),
                total_processing_time_ms=int((time.time() - start_time) * 1000),
                errors=[f"Text is not processable: {processable_result['reason']}"],
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_reprocess:
            cached_result = self._get_from_cache(content_hash)
            if cached_result:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ID ã‚’æ›´æ–°
                cached_result.message_id = message_id
                cached_result.processed_at = datetime.now()

                # çµ±è¨ˆæ›´æ–°
                self.stats.update_stats(cached_result)

                return cached_result

        # AI å‡¦ç†å®Ÿè¡Œ
        errors = []
        warnings: list[str] = []
        summary = None
        tags = None
        category = None

        try:
            self.logger.info(
                "Starting AI processing",
                message_id=message_id,
                text_length=len(cleaned_text),
                content_hash=content_hash,
            )

            # ä¸¦åˆ— AI å‡¦ç†
            if (
                self.settings.enable_summary
                or self.settings.enable_tags
                or self.settings.enable_categorization
            ):
                summary_result, tags_result, category_result = await asyncio.wait_for(
                    self.gemini_client.process_all(cleaned_text),
                    timeout=self.settings.timeout_seconds,
                )

                # çµæœã®å‰²ã‚Šå½“ã¦
                if self.settings.enable_summary:
                    summary = summary_result
                if self.settings.enable_tags:
                    tags = tags_result
                if self.settings.enable_categorization:
                    category = category_result

        except TimeoutError:
            error_msg = (
                f"Processing timeout after {self.settings.timeout_seconds} seconds"
            )
            errors.append(error_msg)
            self.logger.error(error_msg, message_id=message_id)

        except GeminiAPIError as e:
            error_msg = f"Gemini API error: {str(e)}"
            errors.append(error_msg)
            self.logger.error(
                error_msg,
                message_id=message_id,
                error_code=getattr(e, "error_code", None),
            )

        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            errors.append(error_msg)
            self.logger.error(error_msg, message_id=message_id, exc_info=True)

        # çµæœä½œæˆ
        total_time = int((time.time() - start_time) * 1000)

        result = AIProcessingResult(
            message_id=message_id,
            processed_at=datetime.now(),
            summary=summary,
            tags=tags,
            category=category,
            total_processing_time_ms=total_time,
            cache_hit=False,
            errors=errors,
            warnings=warnings,
        )

        # æˆåŠŸã—ãŸå ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if not errors:
            self._save_to_cache(content_hash, result)

        # çµ±è¨ˆæ›´æ–°
        self.stats.update_stats(result)

        self.logger.info(
            "AI processing completed",
            message_id=message_id,
            total_time_ms=total_time,
            has_errors=bool(errors),
            cached=False,
        )

        return result

    async def process_batch(
        self,
        requests: list[ProcessingRequest],
        max_concurrent: int = 3,
        batch_size: int = 5,
    ) -> list[AIProcessingResult]:
        """
        ãƒãƒƒãƒå‡¦ç†ï¼ˆä¸¦åˆ—åŒ–æœ€é©åŒ–ï¼‰

        Args:
            requests: å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

        Returns:
            å‡¦ç†çµæœã®ãƒªã‚¹ãƒˆ
        """
        self.logger.info(
            "Starting optimized batch processing",
            total_requests=len(requests),
            max_concurrent=max_concurrent,
            batch_size=batch_size,
        )

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_requests = sorted(
            requests, key=lambda r: (r.priority.value, r.requested_at), reverse=True
        )

        # ã‚»ãƒãƒ•ã‚©ã‚¢ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        semaphore = asyncio.Semaphore(max_concurrent)
        results = []

        async def process_single_request(
            request: ProcessingRequest,
        ) -> AIProcessingResult:
            """1 ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†"""
            async with semaphore:
                try:
                    return await self.process_text(
                        text=request.text_content,
                        message_id=request.message_id,
                        force_reprocess=request.force_reprocess,
                    )
                except Exception as e:
                    self.logger.error(
                        "Batch processing item failed",
                        message_id=request.message_id,
                        error=str(e),
                    )
                    # ã‚¨ãƒ©ãƒ¼çµæœã‚’ä½œæˆ
                    return AIProcessingResult(
                        message_id=request.message_id,
                        processed_at=datetime.now(),
                        total_processing_time_ms=0,
                        errors=[f"Batch processing failed: {str(e)}"],
                    )

        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã§ä¸¦åˆ—å‡¦ç†
        for i in range(0, len(sorted_requests), batch_size):
            chunk = sorted_requests[i : i + batch_size]

            self.logger.debug(
                f"Processing chunk {i // batch_size + 1}",
                chunk_size=len(chunk),
                total_chunks=(len(sorted_requests) + batch_size - 1) // batch_size,
            )

            # ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
            chunk_tasks = [
                asyncio.create_task(process_single_request(request))
                for request in chunk
            ]

            # çµæœã‚’åé›†
            chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

            # ä¾‹å¤–å‡¦ç†
            for j, result in enumerate(chunk_results):
                if isinstance(result, Exception):
                    error_result = AIProcessingResult(
                        message_id=chunk[j].message_id,
                        processed_at=datetime.now(),
                        total_processing_time_ms=0,
                        errors=[f"Async processing failed: {str(result)}"],
                    )
                    results.append(error_result)
                elif isinstance(result, AIProcessingResult):
                    results.append(result)
                else:
                    # äºˆæœŸã—ãªã„å‹ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†
                    error_result = AIProcessingResult(
                        message_id=chunk[j].message_id,
                        processed_at=datetime.now(),
                        total_processing_time_ms=0,
                        errors=[f"Unexpected result type: {type(result)}"],
                    )
                    results.append(error_result)

        self.logger.info(
            "Optimized batch processing completed",
            total_results=len(results),
            success_count=sum(1 for r in results if not r.errors),
            error_count=sum(1 for r in results if r.errors),
        )
        return results

    def add_to_queue(self, request: ProcessingRequest) -> None:
        """å‡¦ç†ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        self._processing_queue.append(request)
        self.logger.debug(
            "Request added to queue",
            message_id=request.message_id,
            queue_size=len(self._processing_queue),
        )

    async def process_queue(self) -> list[AIProcessingResult]:
        """ã‚­ãƒ¥ãƒ¼ã®å‡¦ç†"""
        if self._is_processing or not self._processing_queue:
            return []

        self._is_processing = True

        try:
            # ã‚­ãƒ¥ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã‚¯ãƒªã‚¢
            requests_to_process = self._processing_queue.copy()
            self._processing_queue.clear()

            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            results = await self.process_batch(requests_to_process)

            return results

        finally:
            self._is_processing = False

    def get_stats(self) -> ProcessingStats:
        """å‡¦ç†çµ±è¨ˆã‚’å–å¾—"""
        # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._clean_expired_cache()

        return self.stats

    def get_cache_info(self) -> dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’å–å¾—ï¼ˆ LRU æœ€é©åŒ–ï¼‰"""
        self._clean_expired_cache()

        # LRU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        cache_stats = self._cache.get_performance_stats()

        return {
            "total_entries": cache_stats["size"],
            "max_entries": cache_stats["max_size"],
            "usage_ratio": cache_stats["usage_ratio"],
            "cache_hits": cache_stats["hits"],
            "cache_misses": cache_stats["misses"],
            "cache_hit_rate": cache_stats["hit_ratio"],
            "is_full": cache_stats["is_full"],
            "ttl_hours": cache_stats["ttl_seconds"] / 3600
            if cache_stats["ttl_seconds"]
            else None,
        }

    def clear_cache(self) -> int:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆ LRU æœ€é©åŒ–ï¼‰"""
        cleared_count = self._cache.size()
        self._cache.clear()
        self._cache.reset_stats()  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚‚ãƒªã‚»ãƒƒãƒˆ
        self.logger.info(f"Cache cleared: {cleared_count} entries removed")
        return cleared_count

    async def generate_embeddings(self, text: str) -> list[float] | None:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ

        Args:
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤±æ•—æ™‚ã¯ None ï¼‰
        """
        try:
            self.logger.debug("Generating embeddings", text_length=len(text))

            # Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            if hasattr(self.gemini_client, "generate_embeddings"):
                embedding = await self.gemini_client.generate_embeddings(text)
                if embedding and isinstance(embedding, list):
                    return list(embedding)

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ãƒ€ãƒŸãƒ¼åŸ‹ã‚è¾¼ã¿
            text_hash = hashlib.sha256(text.encode()).hexdigest()
            embedding = [
                float(int(text_hash[i : i + 2], 16)) / 255.0 for i in range(0, 32, 2)
            ]

            self.logger.warning("Using fallback embedding generation")
            return embedding

        except Exception as e:
            self.logger.error("Failed to generate embeddings", error=str(e))
            return None

    async def summarize_url_content(self, url: str, content: str) -> str | None:
        """
        URL å†…å®¹ã‚’è¦ç´„

        Args:
            url: å¯¾è±¡ URL
            content: ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¤±æ•—æ™‚ã¯ None ï¼‰
        """
        try:
            self.logger.debug(
                "Summarizing URL content", url=url, content_length=len(content)
            )

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""ä»¥ä¸‹ã®ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š

URL: {url}

å†…å®¹:
{content[:2000]}...  # æœ€åˆã® 2000 æ–‡å­—ã®ã¿

è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§ï¼š
- ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ 3-5 è¡Œã§
- é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹
- èª­ã¿ã‚„ã™ã„æ–‡ç« ã§"""

            summary = await self.gemini_client.generate_summary(prompt)

            if summary:
                self.logger.debug("URL content summarized successfully", url=url)
                return summary.summary.strip()

            return None

        except Exception as e:
            self.logger.error("Failed to summarize URL content", url=url, error=str(e))
            return None

    async def generate_internal_links(
        self, content: str, related_notes: list[dict[str, Any]]
    ) -> list[str]:
        """
        å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ

        Args:
            content: æ–°è¦ãƒãƒ¼ãƒˆã®å†…å®¹
            related_notes: é–¢é€£ãƒãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ

        Returns:
            å†…éƒ¨ãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.debug(
                "Generating internal links",
                content_length=len(content),
                related_notes_count=len(related_notes),
            )

            if not related_notes:
                return []

            # é–¢é€£ãƒãƒ¼ãƒˆæƒ…å ±ã‚’æ•´ç†
            note_info = []
            for note in related_notes[:10]:  # ä¸Šä½ 10 ä»¶ã¾ã§
                title = note.get("title", "Untitled")
                similarity = note.get("similarity_score", 0.0)
                preview = note.get("content_preview", "")[:100]

                note_info.append(f"- {title} (é¡ä¼¼åº¦: {similarity:.2f}): {preview}")

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""ä»¥ä¸‹ã®æ–°ã—ã„ãƒãƒ¼ãƒˆã®å†…å®¹ã«åŸºã¥ã„ã¦ã€é–¢é€£ã™ã‚‹æ—¢å­˜ã®ãƒãƒ¼ãƒˆã¸ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š

æ–°ã—ã„ãƒãƒ¼ãƒˆã®å†…å®¹:
{content[:1000]}...

é–¢é€£ã™ã‚‹æ—¢å­˜ã®ãƒãƒ¼ãƒˆ:
{chr(10).join(note_info)}

ä»¥ä¸‹ã®æ¡ä»¶ã§å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š
1. æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ 3-5 å€‹ã®ãƒãƒ¼ãƒˆã‚’é¸æŠ
2. [[ãƒãƒ¼ãƒˆå]]ã®å½¢å¼ã§ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
3. ç°¡æ½”ãªèª¬æ˜ã‚’ä»˜ã‘ã‚‹
4. é–¢é€£æ€§ãŒä½ã„ã‚‚ã®ã¯é™¤å¤–

å‡ºåŠ›å½¢å¼:
- [[ãƒãƒ¼ãƒˆå 1]] - èª¬æ˜
- [[ãƒãƒ¼ãƒˆå 2]] - èª¬æ˜"""

            response = await self.gemini_client.generate_summary(prompt)

            if not response:
                return []

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            links = []
            response_text = (
                response.summary if hasattr(response, "summary") else str(response)
            )
            for line in response_text.split("\n"):
                if "[[" in line and "]]" in line:
                    links.append(line.strip())

            self.logger.debug("Internal links generated", links_count=len(links))
            return links

        except Exception as e:
            self.logger.error("Failed to generate internal links", error=str(e))
            return []

    async def health_check(self) -> dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            test_text = "Hello, this is a test message for health check."
            start_time = time.time()

            await self.gemini_client.generate_summary(test_text)

            response_time = int((time.time() - start_time) * 1000)

            return {
                "status": "healthy",
                "response_time_ms": response_time,
                "model": self.model_config.model_name,
                "cache_entries": len(self._cache),
                "queue_size": len(self._processing_queue),
                "total_requests": self.stats.total_requests,
                "success_rate": self.stats.successful_requests
                / max(self.stats.total_requests, 1),
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "model": self.model_config.model_name,
            }
