"""
Advanced note analyzer with semantic search and AI-powered features
"""

import re
from datetime import datetime
from typing import Any, Union

from src.ai.mock_processor import MockAIProcessor
from src.ai.processor import AIProcessor
from src.ai.url_processor import URLContentExtractor
from src.ai.vector_store import SemanticSearchResult, VectorStore
from src.obsidian.file_manager import ObsidianFileManager
from src.utils.mixins import LoggerMixin

# Settings loaded lazily to avoid circular imports


class AdvancedNoteAnalyzer(LoggerMixin):
    """é«˜åº¦ãªãƒãƒ¼ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        obsidian_file_manager: "ObsidianFileManager",
        ai_processor: Union["AIProcessor", "MockAIProcessor"],
    ):
        """
        åˆæœŸåŒ–

        Args:
            obsidian_file_manager: Obsidian ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            ai_processor: AI å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
        """
        self.file_manager = obsidian_file_manager
        self.ai_processor = ai_processor

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.vector_store = VectorStore(
            obsidian_file_manager=obsidian_file_manager, ai_processor=ai_processor
        )
        self.url_extractor = URLContentExtractor()

        # è¨­å®š
        self.max_related_notes = 10
        self.min_similarity_threshold = 0.3
        self.max_internal_links = 5

        self.logger.info("Advanced note analyzer initialized")

    async def analyze_note_content(
        self,
        content: str,
        title: str,
        file_path: str | None = None,
        include_url_processing: bool = True,
        include_related_notes: bool = True,
        discord_metadata: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """
        ãƒãƒ¼ãƒˆå†…å®¹ã®åŒ…æ‹¬çš„ãªåˆ†æ

        Args:
            content: ãƒãƒ¼ãƒˆå†…å®¹
            title: ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            include_url_processing: URL å‡¦ç†ã‚’å«ã‚€ã‹ã©ã†ã‹
            include_related_notes: é–¢é€£ãƒãƒ¼ãƒˆåˆ†æã‚’å«ã‚€ã‹ã©ã†ã‹
            discord_metadata: Discordç”±æ¥ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ãªã©ï¼‰

        Returns:
            åˆ†æçµæœ
        """
        try:
            self.logger.info(
                "Starting comprehensive note analysis",
                title=title,
                content_length=len(content),
                file_path=file_path,
                has_discord_metadata=bool(discord_metadata),
            )

            analysis_results = {}

            # 0. Discord ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸåˆ†é¡åˆ†æ
            if discord_metadata:
                discord_analysis = await self._analyze_discord_context(
                    content, discord_metadata
                )
                analysis_results["discord_analysis"] = discord_analysis

            # 1. URL å†…å®¹å‡¦ç†ã¨è¦ç´„
            if include_url_processing:
                url_results = await self._process_urls_in_content(content)
                analysis_results["url_processing"] = url_results

                # URL è¦ç´„ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«çµ±åˆ
                if url_results.get("summaries"):
                    content = await self._integrate_url_summaries(
                        content, url_results["summaries"]
                    )

            # 2. é–¢é€£ãƒãƒ¼ãƒˆåˆ†æ
            related_notes = []
            if include_related_notes:
                related_notes = await self._find_related_notes(
                    content, exclude_file=file_path
                )
                analysis_results["related_notes"] = {
                    "results": [
                        {
                            "file_path": note.file_path,
                            "title": note.title,
                            "similarity_score": note.similarity_score,
                            "content_preview": note.content_preview,
                        }
                        for note in related_notes
                    ]
                }

            # 3. å†…éƒ¨ãƒªãƒ³ã‚¯ææ¡ˆ
            internal_links = []
            if related_notes:
                internal_links = await self._generate_internal_links(
                    content, related_notes
                )
                analysis_results["internal_links"] = {"suggestions": internal_links}

            # 4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€çµ‚çµ±åˆ
            enhanced_content = await self._enhance_content_with_links(
                content,
                internal_links,
                analysis_results.get("url_processing", {}),
                discord_metadata,
            )
            analysis_results["enhanced_content"] = {"content": enhanced_content}

            # 5. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆæ–°è¦ãƒãƒ¼ãƒˆã®å ´åˆï¼‰
            if file_path and enhanced_content:
                await self._add_to_vector_store(file_path, title, enhanced_content)

            analysis_results.update(
                {
                    "original_content": {"content": content},
                    "title": {"value": title},
                    "file_path": {"path": file_path or ""},
                    "analyzed_at": {"timestamp": datetime.now().isoformat()},
                    "content_stats": self._get_content_stats(content),
                }
            )

            self.logger.info(
                "Note analysis completed",
                title=title,
                related_notes_count=len(related_notes),
                internal_links_count=len(internal_links),
                urls_processed=len(
                    analysis_results.get("url_processing", {}).get("processed_urls", [])
                ),
                has_discord_context=bool(discord_metadata),
            )

            return analysis_results

        except Exception as e:
            self.logger.error(
                "Failed to analyze note content",
                title=title,
                error=str(e),
                exc_info=True,
            )
            return {
                "error": str(e),
                "title": title,
                "file_path": file_path,
                "analyzed_at": datetime.now().isoformat(),
            }

    async def search_related_notes(
        self, query: str, limit: int = 10, min_similarity: float = 0.1
    ) -> list[dict[str, Any]]:
        """
        é–¢é€£ãƒãƒ¼ãƒˆã‚’æ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            limit: çµæœæ•°åˆ¶é™
            min_similarity: æœ€å°é¡ä¼¼åº¦

        Returns:
            æ¤œç´¢çµæœ
        """
        try:
            results = await self.vector_store.search_similar_notes(
                query_text=query, limit=limit, min_similarity=min_similarity
            )

            return [
                {
                    "file_path": result.file_path,
                    "title": result.title,
                    "similarity_score": result.similarity_score,
                    "content_preview": result.content_preview,
                    "metadata": result.metadata,
                }
                for result in results
            ]

        except Exception as e:
            self.logger.error("Failed to search related notes", error=str(e))
            return []

    async def rebuild_vector_index(self, force: bool = False) -> dict[str, Any]:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰

        Args:
            force: å¼·åˆ¶å†æ§‹ç¯‰

        Returns:
            å†æ§‹ç¯‰çµæœ
        """
        try:
            self.logger.info("Starting vector index rebuild", force=force)

            start_time = datetime.now()
            await self.vector_store.build_index(force_rebuild=force)
            end_time = datetime.now()

            duration = (end_time - start_time).total_seconds()
            stats = await self.vector_store.get_embedding_stats()

            result = {
                "success": True,
                "duration_seconds": duration,
                "stats": stats,
                "rebuilt_at": end_time.isoformat(),
            }

            self.logger.info(
                "Vector index rebuild completed",
                duration=duration,
                embeddings=stats.get("total_embeddings", 0),
            )

            return result

        except Exception as e:
            self.logger.error(
                "Failed to rebuild vector index", error=str(e), exc_info=True
            )
            return {
                "success": False,
                "error": str(e),
                "attempted_at": datetime.now().isoformat(),
            }

    async def _process_urls_in_content(self, content: str) -> dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã® URL ã‚’å‡¦ç†"""
        try:
            # URL ã‚’æŠ½å‡ºãƒ»å‡¦ç†
            url_results = await self.url_extractor.process_urls_in_text(
                content, max_urls=3
            )

            if not url_results.get("processed_urls"):
                return url_results

            # å„ URL ã®å†…å®¹ã‚’è¦ç´„
            summaries = []
            for url_data in url_results["processed_urls"]:
                try:
                    summary = await self.ai_processor.summarize_url_content(
                        url_data["url"], url_data["content"]
                    )

                    if summary:
                        summaries.append(
                            {
                                "url": url_data["url"],
                                "title": url_data["title"],
                                "summary": summary,
                                "original_content_length": url_data["content_length"],
                            }
                        )

                except Exception as e:
                    self.logger.warning(
                        "Failed to summarize URL content",
                        url=url_data["url"],
                        error=str(e),
                    )

            url_results["summaries"] = summaries
            return url_results

        except Exception as e:
            self.logger.error("Failed to process URLs in content", error=str(e))
            return {"error": str(e)}

    async def _integrate_url_summaries(
        self, content: str, summaries: list[dict[str, Any]]
    ) -> str:
        """URL è¦ç´„ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«çµ±åˆï¼ˆæœ‰åŠ¹ãª URL ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
        try:
            if not summaries:
                return content

            # æ—¢å­˜ã® URL è¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹
            if "## ğŸ“ URL è¦ç´„" in content:
                self.logger.debug(
                    "URL è¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
                )
                return content

            # æœ‰åŠ¹ãªè¦ç´„ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            valid_summaries = []
            for summary_data in summaries:
                summary_text = summary_data.get("summary", "").strip()
                url = summary_data.get("url", "").strip()

                # æœ‰åŠ¹ãª URL ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ Discord ç„¡åŠ¹ãƒªãƒ³ã‚¯ãªã©ã‚’é™¤å¤–ï¼‰
                is_valid_url = (
                    url
                    and not url.endswith("/channels/")  # Discord ç„¡åŠ¹ãƒªãƒ³ã‚¯
                    and "discord.com/channels/" not in url  # Discord ä¸å®Œå…¨ãƒªãƒ³ã‚¯
                    and summary_text
                    and not summary_text.startswith(
                        "Discord ã®ä¼šè©±ã®è¦ç´„ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                    )
                    and "URL ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒãªã„" not in summary_text
                    and "ç®‡æ¡æ›¸ã 3 ç‚¹ã«ã‚ˆã‚‹è¦ç´„ã‚’ä½œæˆã§ãã¾ã›ã‚“" not in summary_text
                    and "URL ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦è¦ç´„ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“"
                    not in summary_text
                    and "æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã¯" not in summary_text
                    and "ä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ­£ç¢ºãªè¦ç´„ã¯ã§ãã¾ã›ã‚“" not in summary_text
                )

                if is_valid_url:
                    valid_summaries.append(summary_data)

            # æœ‰åŠ¹ãªè¦ç´„ãŒãªã„å ´åˆã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ãªã„
            if not valid_summaries:
                self.logger.debug("æœ‰åŠ¹ãª URL è¦ç´„ãŒãªã„ãŸã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¾ã›ã‚“")
                return content

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ«å°¾ã« URL è¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            url_section_parts = ["\n\n## ğŸ“ URL è¦ç´„\n"]

            for summary_data in valid_summaries:
                url_section_parts.append(
                    f"### {summary_data['title']}\n"
                    f"ğŸ”— {summary_data['url']}\n\n"
                    f"{summary_data['summary']}\n"
                )

            return content + "".join(url_section_parts)

        except Exception as e:
            self.logger.warning("Failed to integrate URL summaries", error=str(e))
            return content

    async def _find_related_notes(
        self, content: str, exclude_file: str | None = None
    ) -> list[SemanticSearchResult]:
        """é–¢é€£ãƒãƒ¼ãƒˆã‚’æ¤œç´¢"""
        try:
            exclude_files = {exclude_file} if exclude_file else set()

            results = await self.vector_store.search_similar_notes(
                query_text=content,
                limit=self.max_related_notes,
                min_similarity=self.min_similarity_threshold,
                exclude_files=exclude_files,
            )

            return results

        except Exception as e:
            self.logger.error("Failed to find related notes", error=str(e))
            return []

    async def _generate_internal_links(
        self, content: str, related_notes: list[SemanticSearchResult]
    ) -> list[str]:
        """å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ"""
        try:
            if not related_notes:
                return []

            # SemanticSearchResult ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            related_notes_dict = [
                {
                    "title": note.title,
                    "similarity_score": note.similarity_score,
                    "content_preview": note.content_preview,
                    "file_path": note.file_path,
                }
                for note in related_notes[: self.max_internal_links]
            ]

            links = await self.ai_processor.generate_internal_links(
                content, related_notes_dict
            )

            return links

        except Exception as e:
            self.logger.error("Failed to generate internal links", error=str(e))
            return []

    async def _enhance_content_with_links(
        self,
        content: str,
        internal_links: list[str],
        url_processing_results: dict[str, Any],
        discord_metadata: dict[str, Any] | None = None,
    ) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ã¦å¼·åŒ–"""
        try:
            enhanced_content = content

            # Discord ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            if discord_metadata:
                discord_section = await self._create_discord_metadata_section(
                    discord_metadata
                )
                if discord_section:
                    enhanced_content = discord_section + "\n\n" + enhanced_content

            # URL è¦ç´„ãŒæ—¢ã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if url_processing_results.get("summaries"):
                url_summaries = url_processing_results["summaries"]
                enhanced_content = await self._integrate_url_summaries(
                    enhanced_content, url_summaries
                )

            # å†…éƒ¨ãƒªãƒ³ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            if internal_links:
                links_section = "\n\n## ğŸ”— é–¢é€£ãƒãƒ¼ãƒˆ\n\n"
                links_section += "\n".join(internal_links)
                enhanced_content += links_section

            return enhanced_content

        except Exception as e:
            self.logger.warning("Failed to enhance content with links", error=str(e))
            return content

    async def _analyze_discord_context(
        self, content: str, discord_metadata: dict[str, Any]
    ) -> dict[str, Any]:
        """Discord ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡ã‚’å¼·åŒ–"""
        try:
            # Discord ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            channel_name = discord_metadata.get("channel_name", "unknown")
            channel_category = discord_metadata.get("channel_category", "unknown")
            message_timestamp = discord_metadata.get("timestamp")
            user_id = discord_metadata.get("user_id")

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã®æ¨å®š
            content_type = await self._infer_content_type_from_context(
                content, channel_name, channel_category
            )

            # é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã®æ¨å®š
            suggested_folder = await self._suggest_folder_from_context(
                content, content_type, channel_category
            )

            # ã‚¿ã‚°ã®ææ¡ˆ
            suggested_tags = await self._suggest_tags_from_context(
                content, channel_category, content_type
            )

            return {
                "content_type": content_type,
                "suggested_folder": suggested_folder,
                "suggested_tags": suggested_tags,
                "channel_context": {
                    "name": channel_name,
                    "category": channel_category,
                    "timestamp": message_timestamp,
                    "user_id": user_id,
                },
                "classification_confidence": await self._calculate_classification_confidence(
                    content, content_type, suggested_folder
                ),
            }

        except Exception as e:
            self.logger.error("Failed to analyze Discord context", error=str(e))
            return {"error": str(e)}

    async def _infer_content_type_from_context(
        self, content: str, channel_name: str, channel_category: str
    ) -> str:
        """ãƒãƒ£ãƒ³ãƒãƒ«åã¨ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•ã¨ç‰¹å¾´ã‚’åˆ†æ
        content_lower = content.lower()

        # 1. Archive ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ€å„ªå…ˆ - å®Œäº†ãƒ»éå»ã®åˆ¤å®šï¼‰
        archive_keywords = [
            "å®Œäº†ã—ãŸ",
            "å®Œæˆã—ãŸ",
            "çµ‚äº†ã—ãŸ",
            "ä¿®äº†ã—ãŸ",
            "éå»ã®",
            "æ˜¨å¹´ã®",
            "å‰ã®",
            "ä»¥å‰ã®",
            "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–",
            "å±¥æ­´",
            "è¨¼æ˜æ›¸",
            "å¥‘ç´„æ›¸",
            "å ±å‘Šæ›¸",
            "æœ€çµ‚",
        ]
        # ã€Œè¨˜éŒ²ã€ã¯ç¾åœ¨é€²è¡Œå½¢ã®å ´åˆArchiveå¯¾è±¡å¤–
        has_current_record = any(
            indicator in content_lower
            for indicator in ["ä»Šæ—¥", "ç¾åœ¨", "ç¶™ç¶šä¸­", "ç›®æ¨™"]
        )
        if not has_current_record and "è¨˜éŒ²" in content_lower:
            archive_keywords.append("è¨˜éŒ²")

        if any(keyword in content_lower for keyword in archive_keywords):
            return "archive"

        # 2. Learning ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¶…é«˜å„ªå…ˆ - å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¢ºå®Ÿãªè­˜åˆ¥ï¼‰
        # ã€ŒåŸºæœ¬ï¼šã€ã€Œã«ã¤ã„ã¦ã€ãªã©ã®å­¦ç¿’ç‰¹æœ‰è¡¨ç¾ã‚’æœ€å„ªå…ˆ
        learning_keywords = [
            "å­¦ç¿’",
            "èª­æ›¸",
            "å‹‰å¼·",
            "research",
            "python",
            "æ©Ÿæ¢°å­¦ç¿’",
            "æ•™å¸«ã‚ã‚Š",
            "éåŒæœŸå‡¦ç†",
            "async",
            "await",
            "ã«ã¤ã„ã¦å­¦ã‚“ã ",
            "ä½¿ç”¨ã—ã¦",
            "è¨“ç·´ã™ã‚‹",
        ]
        # å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¼·åŠ›ãªè­˜åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        strong_learning_patterns = [
            "åŸºæœ¬ï¼š",
            "ã®åŸºæœ¬",
            "ã«ã¤ã„ã¦å­¦ã‚“ã ",
            "é‡è¦æ€§ã«ã¤ã„ã¦",
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®åŸºæœ¬",
            "è¨­å®šã€ä¾¡å€¤ææ¡ˆã€ãƒãƒ£ãƒãƒ«",
            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®šã€ä¾¡å€¤ææ¡ˆ",
            "é¸æŠã®é‡è¦æ€§",
        ]
        has_strong_learning = any(
            pattern in content_lower for pattern in strong_learning_patterns
        )
        if has_strong_learning or any(
            keyword in content_lower for keyword in learning_keywords
        ):
            return "learning"

        # 3. Task ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¶…é«˜å„ªå…ˆ - æœŸé™ä»˜ãã‚¿ã‚¹ã‚¯ã®ç¢ºå®Ÿãªè­˜åˆ¥ï¼‰
        # ã€Œã¾ã§ã«ã€ã€Œå¿…è¦ãŒã‚ã‚Šã¾ã™ã€ãªã©ã®æœŸé™ãƒ»å¿…è¦æ€§è¡¨ç¾ã‚’æœ€å„ªå…ˆ
        urgent_task_patterns = [
            "ã¾ã§ã«",
            "å¿…è¦ãŒã‚ã‚Šã¾ã™",
            "ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„",
            "æ˜æ—¥ã¾ã§ã«",
            "æ¥é€±ã¾ã§",
            "æœŸé™",
            "deadline",
        ]
        task_keywords = [
            "todo",
            "ã‚„ã‚‹ã“ã¨",
            "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«",
            "æº–å‚™",
            "ä½œæˆ",
            "é€£çµ¡",
            "æ‰‹ç¶šãã‚’æ¸ˆã¾ã›ã‚‹",
            "ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹",
            "è¿”ä¿¡ã‚’ã™ã‚‹",
            "æº–å‚™ã‚’ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„",
            "æº–å‚™ãŒå¿…è¦",
            "é€£çµ¡ãŒå¿…è¦",
        ]

        # æœŸé™ä»˜ãã‚¿ã‚¹ã‚¯ã¯ä»–ã®ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªã‚ˆã‚Šå„ªå…ˆ
        has_urgent_task = any(
            pattern in content_lower for pattern in urgent_task_patterns
        )
        if (
            has_urgent_task
            or channel_category == "productivity"
            or any(keyword in content_lower for keyword in task_keywords)
        ):
            return "task"

        # 4. Finance ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜å„ªå…ˆ - é‡‘èé–¢é€£ï¼‰
        financial_keywords = [
            "å††",
            "Â¥",
            "$",
            "æ”¯å‡º",
            "åå…¥",
            "è³¼å…¥",
            "æ”¯æ‰•ã„",
            "æŠ•è³‡",
            "è²¯é‡‘",
            "å®¶è¨ˆç°¿",
            "é£Ÿè²»",
            "äº¤é€šè²»",
            "å…‰ç†±è²»",
            "æ˜¼é£Ÿ",
            "ã‚³ãƒ¼ãƒ’ãƒ¼",
            "ä»£",
        ]
        financial_context = any(
            keyword in content_lower for keyword in ["å®¶è¨ˆç°¿", "æ”¯å‡º", "ä»Šæ—¥ã®æ”¯å‡º"]
        )
        if (
            channel_category == "finance"
            or financial_context
            or any(keyword in content_lower for keyword in financial_keywords)
        ):
            # ã€Œæ›¸ç±ã€å˜ä½“ã¯é™¤å¤–ã€ã€Œæ›¸ç±ä»£ã€ãªã©ã®æ”¯å‡ºæ–‡è„ˆã®ã¿Finance
            if "æ›¸ç±" in content_lower and not any(
                expense in content_lower for expense in ["ä»£", "æ”¯å‡º", "å††"]
            ):
                pass  # æ¬¡ã®åˆ¤å®šã«æµã™
            else:
                return "finance"

        # 5. Health ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜å„ªå…ˆ - å¥åº·è¨˜éŒ²ï¼‰
        health_keywords = [
            "ä½“é‡",
            "è¡€åœ§",
            "é‹å‹•",
            "ç¡çœ ",
            "å¥åº·",
            "workout",
            "fitness",
            "ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°",
            "km",
            "bpm",
            "é£Ÿäº‹åˆ¶é™",
            "ç¶™ç¶šä¸­",
            "ç›®è¦šã‚",
            "kg",
            "ç›®æ¨™",
        ]
        health_context = any(
            keyword in content_lower
            for keyword in ["ä½“é‡è¨˜éŒ²", "ç¡çœ ãƒ­ã‚°", "ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°"]
        )
        if (
            channel_category == "health"
            or health_context
            or any(keyword in content_lower for keyword in health_keywords)
        ):
            return "health"

        # 6. Ideas ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‰µä½œãƒ»ç™ºæƒ³ã®åˆ¤å®šï¼‰
        idea_keywords = [
            "ã‚¢ã‚¤ãƒ‡ã‚¢",
            "ãƒã‚¿",
            "ç€æƒ³",
            "æ€ã„ã¤ã",
            "è€ƒãˆ",
            "ç™ºæƒ³",
            "å‰µä½œ",
            "ä¼ç”»æ¡ˆ",
            "ã‚³ãƒ³ã‚»ãƒ—ãƒˆ",
            "æ§‹æƒ³",
            "æ¡ˆ",
            "ææ¡ˆ",
        ]
        if any(keyword in content_lower for keyword in idea_keywords):
            return "idea"

        # 7. Projects ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä»•äº‹ãƒ»é–‹ç™ºã®åˆ¤å®šï¼‰
        project_keywords = [
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "æ¡ˆä»¶",
            "é–‹ç™º",
            "åˆ¶ä½œ",
            "é€²æ—",
            "ãƒ•ã‚§ãƒ¼ã‚º",
            "çµ±åˆ",
            "å®Ÿè£…",
            "äº‹æ¥­",
            "ä¼ç”»",
            "è¨ˆç”»",
            "æ¥­å‹™",
            "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ",
            "é¡§å®¢",
            "å—æ³¨",
        ]
        if any(keyword in content_lower for keyword in project_keywords):
            return "project"

        # 8. Resources ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‚è€ƒè³‡æ–™ã®åˆ¤å®šï¼‰
        resource_keywords = [
            "å‚è€ƒ",
            "è³‡æ–™",
            "æ–‡çŒ®",
            "è«–æ–‡",
            "æ›¸ç±",
            "æœ¬",
            "è¨˜äº‹",
            "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
            "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ",
            "URL",
            "ãƒªãƒ³ã‚¯",
            "ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
            "ã‚¬ã‚¤ãƒ‰",
            "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«",
            "https://",
            "http://",
        ]
        if any(keyword in content_lower for keyword in resource_keywords):
            return "resource"

        # 9. Daily ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ—¥å¸¸è¨˜éŒ²ã®åˆ¤å®šï¼‰
        daily_keywords = [
            "ä»Šæ—¥",
            "æ˜¨æ—¥",
            "ä»Šæœ",
            "ä»Šå¤œ",
            "æœã‹ã‚‰",
            "å¤•æ–¹",
            "æ•£æ­©",
            "å¤©æ°—",
            "å‹é”",
            "å®¶æ—",
            "æ˜ ç”»",
            "æ–™ç†",
            "æŒ¯ã‚Šè¿”ã‚Š",
            "æ—¥è¨˜",
            "å‡ºæ¥äº‹",
        ]
        if any(keyword in content_lower for keyword in daily_keywords):
            return "daily"

        # 10. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®š
        if len(content) < 100:
            return "quick_note"
        else:
            return "memo"

    async def _suggest_folder_from_context(
        self, content: str, content_type: str, channel_category: str
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ã‚’ææ¡ˆ"""
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã«åŸºã¥ããƒ•ã‚©ãƒ«ãƒ€ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ•°å€¤ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹çµ±ä¸€ï¼‰
        type_folder_map = {
            # æ–°è¦ã‚«ãƒ†ã‚´ãƒª
            "archive": "ğŸ“¦ 30_Archive",
            "idea": "ğŸ’¡ 03_Ideas",
            "project": "ğŸš€ 11_Projects",
            "resource": "ğŸ“– 12_Resources",
            "daily": "ğŸ“… 01_DailyNotes",
            # æ—¢å­˜ã‚«ãƒ†ã‚´ãƒªï¼ˆçµ±ä¸€ï¼‰
            "finance": "ğŸ’° 20_Finance",
            "task": "âœ… 02_Tasks",
            "health": "ğŸƒ 21_Health",
            "learning": "ğŸ“š 10_Knowledge",
            "quick_note": "ğŸ“¥ 00_Inbox",
            "memo": "ğŸ“¥ 00_Inbox",
        }

        return type_folder_map.get(content_type, "ğŸ“¥ 00_Inbox")

    async def _suggest_tags_from_context(
        self, content: str, channel_category: str, content_type: str
    ) -> list[str]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ææ¡ˆ"""
        tags = []

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°
        if content_type == "finance":
            tags.append("#finance")
            if "æ”¯å‡º" in content.lower() or "è³¼å…¥" in content.lower():
                tags.append("#expense")
            if "åå…¥" in content.lower():
                tags.append("#income")
        elif content_type == "task":
            tags.append("#task")
            if "urgent" in content.lower() or "ç·Šæ€¥" in content.lower():
                tags.append("#urgent")
        elif content_type == "health":
            tags.append("#health")
        elif content_type == "learning":
            tags.append("#learning")

        # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°ï¼ˆä»Šæœˆ/ä»Šå¹´ï¼‰
        now = datetime.now()
        tags.append(f"#{now.strftime('%Y-%m')}")

        return tags

    async def _calculate_classification_confidence(
        self, content: str, content_type: str, suggested_folder: str
    ) -> float:
        """åˆ†é¡ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        # ç°¡å˜ãªä¿¡é ¼åº¦è¨ˆç®—ï¼ˆå®Ÿéš›ã«ã¯ã‚‚ã£ã¨è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…å¯èƒ½ï¼‰
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•ã«åŸºã¥ãèª¿æ•´
        if len(content) > 50:
            confidence += 0.2
        if len(content) > 200:
            confidence += 0.1

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã«åŸºã¥ãèª¿æ•´
        content_lower = content.lower()
        type_keywords = {
            "finance": ["å††", "Â¥", "æ”¯å‡º", "åå…¥", "è³¼å…¥"],
            "task": ["todo", "ã‚¿ã‚¹ã‚¯", "å®Œäº†", "æœŸé™"],
            "health": ["ä½“é‡", "é‹å‹•", "ç¡çœ ", "å¥åº·"],
            "learning": ["å­¦ç¿’", "èª­æ›¸", "å‹‰å¼·"],
        }

        if content_type in type_keywords:
            matches = sum(
                1 for keyword in type_keywords[content_type] if keyword in content_lower
            )
            confidence += min(matches * 0.1, 0.3)

        return min(confidence, 1.0)

    async def _create_discord_metadata_section(
        self, discord_metadata: dict[str, Any]
    ) -> str | None:
        """Discord ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        try:
            if not discord_metadata:
                return None

            channel_name = discord_metadata.get("channel_name")
            timestamp = discord_metadata.get("timestamp")

            if not any([channel_name, timestamp]):
                return None

            metadata_lines = ["## ğŸ“± Discord Info"]

            if channel_name:
                metadata_lines.append(f"- **Channel**: #{channel_name}")
            if timestamp:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚ˆã‚Šèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
                try:
                    dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
                    formatted_time = dt.strftime("%Y-%m-%d %H:%M")
                    metadata_lines.append(f"- **Created**: {formatted_time}")
                except Exception:
                    metadata_lines.append(f"- **Created**: {timestamp}")

            return "\n".join(metadata_lines)

        except Exception as e:
            self.logger.warning(
                "Failed to create Discord metadata section", error=str(e)
            )
            return None

    async def _add_to_vector_store(
        self, file_path: str, title: str, content: str
    ) -> None:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒ¼ãƒˆã‚’è¿½åŠ """
        try:
            await self.vector_store.add_note_embedding(
                file_path=file_path, title=title, content=content
            )

        except Exception as e:
            self.logger.warning(
                "Failed to add note to vector store", file_path=file_path, error=str(e)
            )

    def _get_content_stats(self, content: str) -> dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            lines = content.split("\n")
            words = content.split()

            # è¦‹å‡ºã—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            headers = re.findall(r"^#{1,6}\s+.+$", content, re.MULTILINE)

            # ãƒªãƒ³ã‚¯æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            markdown_links = re.findall(r"\[([^\]]+)\]\([^)]+\)", content)
            wiki_links = re.findall(r"\[\[([^\]]+)\]\]", content)

            return {
                "character_count": len(content),
                "word_count": len(words),
                "line_count": len(lines),
                "header_count": len(headers),
                "markdown_link_count": len(markdown_links),
                "wiki_link_count": len(wiki_links),
                "total_link_count": len(markdown_links) + len(wiki_links),
            }

        except Exception as e:
            self.logger.warning("Failed to get content stats", error=str(e))
            return {}

    async def get_system_stats(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            vector_stats = await self.vector_store.get_embedding_stats()

            return {
                "vector_store": vector_stats,
                "analyzer_config": {
                    "max_related_notes": self.max_related_notes,
                    "min_similarity_threshold": self.min_similarity_threshold,
                    "max_internal_links": self.max_internal_links,
                },
                "system_status": "operational",
                "last_updated": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error("Failed to get system stats", error=str(e))
            return {"error": str(e)}
