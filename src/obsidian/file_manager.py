"""Obsidian file manager using modular components."""

import re
from datetime import datetime
from pathlib import Path
from typing import Any

import aiofiles
import structlog

from src.config import get_settings
from src.obsidian.analytics import VaultStatistics
from src.obsidian.backup import BackupConfig, BackupManager
from src.obsidian.core import FileOperations, VaultManager
from src.obsidian.models import FileOperation, ObsidianNote
from src.obsidian.search import NoteSearch, SearchCriteria
from src.utils.mixins import LoggerMixin

logger = structlog.get_logger(__name__)


class ObsidianFileManager(LoggerMixin):
    """
    Unified file manager that orchestrates modular components.

    This class follows the Single Responsibility Principle by delegating
    specific tasks to specialized components while maintaining backward compatibility.
    """

    def __init__(
        self, vault_path: Path | str | None = None, enable_local_data: bool = True
    ):
        """
        Initialize Obsidian file manager with full backward compatibility.

        Args:
            vault_path: Path to Obsidian vault (defaults to settings)
            enable_local_data: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        """
        if vault_path:
            self.vault_path = Path(vault_path)
        else:
            settings = get_settings()
            self.vault_path = settings.obsidian_vault_path

        # æ“ä½œå±¥æ­´ (backward compatibility)
        self.operation_history: list[FileOperation] = []

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (backward compatibility)
        self._folder_cache: set[Path] = set()
        self._stats_cache: Any | None = None
        self._stats_cache_time: datetime | None = None

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç† (backward compatibility)
        self.local_data_manager = None
        if enable_local_data:
            try:
                from src.obsidian.local_data_manager import LocalDataManager

                self.local_data_manager = LocalDataManager(self.vault_path)
            except ImportError:
                self.logger.warning("Local data manager not available")

        # Initialize modern components
        self.file_operations = FileOperations(self.vault_path)
        self.vault_manager = VaultManager(self.vault_path)
        self.note_search = NoteSearch(self.vault_path)
        self.statistics = VaultStatistics(self.vault_path)

        # Initialize backup manager with default config
        backup_config = BackupConfig(
            backup_directory=self.vault_path.parent / "backups",
            max_backups=10,
            compress=True,
        )
        self.backup_manager = BackupManager(self.vault_path, backup_config)

        self.logger.info(
            "Unified Obsidian file manager initialized",
            vault_path=str(self.vault_path),
            local_data_enabled=enable_local_data,
        )

    def _resolve_note_path(
        self, note: ObsidianNote, subfolder: str | None = None
    ) -> Path:
        """Resolve the absolute path for a note within the vault."""
        if note.file_path and note.file_path != Path():
            return (
                note.file_path
                if note.file_path.is_absolute()
                else self.vault_path / note.file_path
            )

        base_folder = self.vault_path / subfolder if subfolder else self.vault_path
        base_folder.mkdir(parents=True, exist_ok=True)

        filename = (
            note.filename
            if note.filename
            else f"{self.file_operations._sanitize_filename(note.title)}.md"
        )
        return base_folder / filename

    # Vault Management
    async def initialize_vault(self) -> bool:
        """Initialize vault structure and templates."""
        return await self.vault_manager.initialize_vault()

    # File Operations
    async def save_note(
        self, note: ObsidianNote, subfolder: str | None = None, overwrite: bool = False
    ) -> Path | None:
        """Save a note to the vault."""
        try:
            if overwrite:
                target_path = self._resolve_note_path(note, subfolder)
                if target_path.exists():
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    if await self.file_operations.update_note(target_path, note):
                        self.statistics.invalidate_cache()
                        return target_path
                    return None
                self.logger.warning(
                    "Overwrite requested but existing file was not found",
                    target=str(target_path),
                )

            saved_path = await self.file_operations.save_note(note, subfolder)
            # Invalidate stats cache when adding new notes
            self.statistics.invalidate_cache()
            return saved_path
        except Exception:
            return None

    async def load_note(self, file_path: Path) -> ObsidianNote | None:
        """Load a note from the vault."""
        return await self.file_operations.load_note(file_path)

    async def update_note(self, file_path: Path, note: ObsidianNote) -> bool:
        """Update an existing note."""
        success = await self.file_operations.update_note(file_path, note)
        if success:
            self.statistics.invalidate_cache()
        return success

    async def append_to_note(
        self,
        file_path: Path,
        content: str,
        section_header: str | None = None,
    ) -> bool:
        """Append content to an existing note."""
        success = await self.file_operations.append_to_note(
            file_path, content, section_header
        )
        if success:
            self.statistics.invalidate_cache()
        return success

    async def delete_note(self, file_path: Path, backup: bool = True) -> bool:
        """Delete a note from the vault."""
        success = await self.file_operations.delete_note(file_path, backup)
        if success:
            self.statistics.invalidate_cache()
        return success

    # Daily Note Integration (preserved for compatibility)
    async def save_or_append_daily_note(
        self,
        note: ObsidianNote,
        target_date: str | None = None,
    ) -> Path:
        """Save or append to daily note."""
        from datetime import date

        # Use provided date or today
        if target_date:
            daily_date = target_date
        else:
            daily_date = date.today().strftime("%Y-%m-%d")

        # Ensure daily notes folder exists
        daily_folder = await self.vault_manager.ensure_folder_exists("Daily Notes")
        daily_file_path = daily_folder / f"{daily_date}.md"

        if daily_file_path.exists():
            # Append to existing daily note
            await self.append_to_note(daily_file_path, note.content, note.title)
            return daily_file_path
        else:
            # Create new daily note
            from src.obsidian.models import NoteFrontmatter

            daily_frontmatter = NoteFrontmatter(
                obsidian_folder="Daily Notes",
                created=daily_date,
                tags=["daily-note"] + (note.frontmatter.tags or []),
                ai_category="Daily",
            )

            daily_note = ObsidianNote(
                filename=f"daily-{daily_date}.md",
                file_path=Path(f"Daily Notes/daily-{daily_date}.md"),
                frontmatter=daily_frontmatter,
                content=f"## {note.title}\n\n{note.content}",
            )
            return await self.file_operations.save_note(daily_note, "Daily Notes")

    # Search Operations
    async def search_notes(
        self,
        query: str | None = None,
        tags: list[str] | None = None,
        category: str | None = None,
        max_results: int = 50,
        folder: str | None = None,
        status: str | None = None,
        date_from: str | None = None,
        date_to: str | None = None,
        limit: int | None = None,
    ) -> list[dict[str, Any]]:
        """Search notes in the vault."""
        # Use limit if provided, otherwise use max_results
        effective_limit = limit if limit is not None else max_results

        criteria = SearchCriteria(
            query=query,
            tags=tags,
            category=category,
            max_results=effective_limit,
            folder=folder,
            status=status,
            date_from=date_from,
            date_to=date_to,
        )

        results = await self.note_search.search_notes(criteria)

        # Convert to dict format for compatibility
        return [result.to_dict() for result in results]

    # Statistics Operations
    async def get_vault_stats(self, force_refresh: bool = False) -> dict[str, Any]:
        """Get comprehensive vault statistics."""
        stats = await self.statistics.get_vault_stats(force_refresh)
        return stats.to_dict()

    # Backup Operations
    async def backup_vault(self, description: str | None = None) -> dict[str, Any]:
        """Create a backup of the vault."""
        result = await self.backup_manager.create_backup(description)
        return result.to_dict()

    async def list_backups(self) -> list[dict[str, Any]]:
        """List available backups."""
        return await self.backup_manager.list_backups()

    async def restore_backup(self, backup_name: str) -> bool:
        """Restore vault from backup."""
        backup_path = self.backup_manager.config.backup_directory / backup_name
        success = await self.backup_manager.restore_backup(backup_path)
        if success:
            self.statistics.invalidate_cache()
        return success

    # Operation History (preserved for compatibility)
    def get_operation_history(self) -> list[dict[str, Any]]:
        """Get file operation history."""
        return self.file_operations.get_operation_history()

    def clear_operation_history(self) -> None:
        """Clear file operation history."""
        self.file_operations.clear_operation_history()

    # Helper Methods for Backwards Compatibility
    async def _ensure_vault_structure(self) -> None:
        """Ensure vault structure (backwards compatibility)."""
        await self.vault_manager.initialize_vault()

    def _invalidate_stats_cache(self) -> None:
        """Invalidate stats cache (backwards compatibility)."""
        self.statistics.invalidate_cache()

    # Configuration
    def configure_backup(self, backup_config: BackupConfig) -> None:
        """Configure backup settings."""
        self.backup_manager = BackupManager(self.vault_path, backup_config)
        logger.info(
            "Backup configuration updated",
            backup_dir=str(backup_config.backup_directory),
        )

    def configure_statistics_cache(self, cache_duration: int) -> None:
        """Configure statistics cache duration."""
        self.statistics = VaultStatistics(self.vault_path, cache_duration)
        logger.info("Statistics cache duration updated", duration=cache_duration)

    # Critical missing methods from original file_manager.py

    def _clean_duplicate_sections(self, new_content: str, existing_content: str) -> str:
        """
        æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰é‡è¤‡ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å»

        Args:
            new_content: æ–°ã—ãè¿½åŠ ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            existing_content: æ—¢å­˜ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

        Returns:
            é‡è¤‡é™¤å»å¾Œã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        try:
            # é‡è¤‡ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            duplicate_patterns = [
                r"## ğŸ“… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿.*?(?=##|\Z)",  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                r"## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯.*?(?=##|\Z)",  # é–¢é€£ãƒªãƒ³ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                r"# ğŸ“\s*\n*",  # é‡è¤‡ã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«
            ]

            cleaned_content = new_content

            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§é‡è¤‡ã‚’é™¤å»
            for pattern in duplicate_patterns:
                # æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŒã˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰é™¤å»
                if re.search(pattern, existing_content, re.DOTALL):
                    cleaned_content = re.sub(
                        pattern, "", cleaned_content, flags=re.DOTALL
                    )

            # URL è¦ç´„ã®é‡è¤‡ã‚’é™¤å»ï¼ˆåŒã˜ URL ã®å ´åˆï¼‰
            existing_urls = re.findall(r"ğŸ”— (https?://[^\s]+)", existing_content)
            for url in existing_urls:
                # åŒã˜ URL ã®è¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å»
                url_section_pattern = (
                    rf"## ğŸ“ URL è¦ç´„.*?### .*?\n ğŸ”— {re.escape(url)}.*?(?=##|\Z)"
                )
                cleaned_content = re.sub(
                    url_section_pattern, "", cleaned_content, flags=re.DOTALL
                )

            # ç©ºè¡Œã®æ•´ç†
            cleaned_content = re.sub(r"\n{3,}", "\n\n", cleaned_content)
            cleaned_content = cleaned_content.strip()

            # å®Œå…¨ã«ç©ºã«ãªã£ãŸå ´åˆã¯å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã®ã¿ä¿æŒ
            if not cleaned_content or cleaned_content.isspace():
                # æœ€å°é™ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆæ™‚åˆ»ã®ã¿ï¼‰ã‚’æŠ½å‡º
                timestamp_match = re.search(r"## \d{2}:\d{2}", new_content)
                if timestamp_match:
                    cleaned_content = timestamp_match.group(0)
                else:
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆ
                    current_time = datetime.now().strftime("%H:%M")
                    cleaned_content = f"## {current_time}"

            return cleaned_content

        except Exception as e:
            self.logger.warning("Failed to clean duplicate sections", error=str(e))
            return new_content  # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™  # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™

    def _parse_markdown_file(self, content: str) -> tuple[dict[str, Any], str]:
        """
        Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åˆ†é›¢

        Args:
            content: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®å†…å®¹

        Returns:
            ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã® dict ã¨ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¿ãƒ—ãƒ«
        """
        try:
            # YAML ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã®æ¤œå‡º
            frontmatter_match = re.match(r"^---\n(.*?)\n---\n(.*)", content, re.DOTALL)

            if frontmatter_match:
                import yaml

                frontmatter_str = frontmatter_match.group(1)
                markdown_content = frontmatter_match.group(2).strip()

                # YAML ã®è§£æ
                frontmatter_data = yaml.safe_load(frontmatter_str) or {}

                return frontmatter_data, markdown_content
            else:
                # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ãŒãªã„å ´åˆ
                return {}, content.strip()

        except Exception as e:
            self.logger.warning("Failed to parse markdown file", error=str(e))
            return {}, content.strip()

    async def _restructure_daily_note(self, note: ObsidianNote) -> str:
        """æ—¥æ¬¡ãƒãƒ¼ãƒˆã®æ§‹é€ ã‚’å†æ§‹ç¯‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        return note.content

    async def _insert_before_metadata(self, content: str, metadata_content: str) -> str:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŒ¿å…¥ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢
        metadata_pattern = r"## ğŸ“… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"
        if re.search(metadata_pattern, content):
            return re.sub(
                metadata_pattern,
                f"{metadata_content}\n\n## ğŸ“… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿",
                content,
                count=1,
            )
        else:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯æœ«å°¾ã«è¿½åŠ 
            return f"{content}\n\n{metadata_content}"

    # Enhanced vault structure management - removing duplicate definition

    async def _create_template_files(self) -> None:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        from src.obsidian.models import VaultFolder

        templates_dir = self.vault_path / VaultFolder.TEMPLATES.value

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        message_template_content = """# {{title}}

## ğŸ“ è¦ç´„
{{ai_summary}}

## ğŸ’¬ å…ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
```
{{original_content}}
```

## ğŸ·ï¸ ã‚¿ã‚°
{{ai_tags}}

## ğŸ“‚ åˆ†é¡
- **ã‚«ãƒ†ã‚´ãƒª**: {{ai_category}}
- **ä¿¡é ¼åº¦**: {{ai_confidence}}

## ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«
{{attachments}}

## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯
- [Discord Message]({{discord_link}})
- **ãƒãƒ£ãƒ³ãƒãƒ«**: #{{channel_name}}

## ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
- **ä½œæˆè€…**: {{author_name}}
- **ä½œæˆæ—¥æ™‚**: {{created_time}}
- **AI å‡¦ç†æ™‚é–“**: {{processing_time}}ms"""

        message_template_path = templates_dir / "message_note_template.md"
        if not message_template_path.exists():
            async with aiofiles.open(message_template_path, "w", encoding="utf-8") as f:
                await f.write(message_template_content)

        # æ—¥æ¬¡ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        daily_template_content = """# Daily Note - {{date}}

## ğŸ“Š ä»Šæ—¥ã®çµ±è¨ˆ
- **ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°**: {{total_messages}}
- **AI å‡¦ç†æ¸ˆã¿**: {{processed_messages}}
- **å‡¦ç†æ™‚é–“åˆè¨ˆ**: {{ai_time_total}}ms

## ğŸ“ ä»Šæ—¥ã®ãƒ¡ãƒ¢

### ä»•äº‹ ({{work_count}}ä»¶)
{{work_notes}}

### å­¦ç¿’ ({{learning_count}}ä»¶)
{{learning_notes}}

### ç”Ÿæ´» ({{life_count}}ä»¶)
{{life_notes}}

### ã‚¢ã‚¤ãƒ‡ã‚¢ ({{ideas_count}}ä»¶)
{{ideas_notes}}

## ğŸ·ï¸ ä»Šæ—¥ã®ã‚¿ã‚°
{{daily_tags}}

## ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«
{{daily_attachments}}"""

        daily_template_path = templates_dir / "daily_note_template.md"
        if not daily_template_path.exists():
            async with aiofiles.open(daily_template_path, "w", encoding="utf-8") as f:
                await f.write(daily_template_content)

    def search_notes_fast(
        self,
        query: str | None = None,
        tags: list[str] | None = None,
        status: str | None = None,
        category: str | None = None,
        limit: int = 50,
    ) -> list[Path]:
        """é«˜é€Ÿãƒãƒ¼ãƒˆæ¤œç´¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨ãƒ»å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        if not self.local_data_manager:
            self.logger.warning(
                "Local data manager not enabled, falling back to regular search"
            )
            return []

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
        file_keys = self.local_data_manager.data_index.search_notes(
            query=query, tags=tags, status=status, category=category, limit=limit
        )

        # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        return [self.vault_path / file_key for file_key in file_keys]

    # Local data management methods (backward compatibility)
    async def initialize_local_data(self) -> None:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–"""
        if self.local_data_manager:
            await self.local_data_manager.initialize()

    async def create_vault_snapshot(self) -> dict[str, Any]:
        """Vault ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆ"""
        if self.local_data_manager:
            snapshot_path = await self.local_data_manager.create_snapshot()
            if snapshot_path:
                return {"snapshot_path": str(snapshot_path)}
        return {}

    async def restore_vault_snapshot(self, snapshot_path: str) -> bool:
        """Vault ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å¾©å…ƒ"""
        if self.local_data_manager:
            return await self.local_data_manager.restore_snapshot(Path(snapshot_path))
        return False

    async def export_vault_data(self, format: str = "json") -> Path | None:
        """Vault ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if self.local_data_manager:
            return await self.local_data_manager.export_vault_data(format)
        return None

    async def sync_with_remote(self, remote_config: dict[str, Any]) -> bool:
        """ãƒªãƒ¢ãƒ¼ãƒˆã¨åŒæœŸ"""
        if self.local_data_manager and "remote_path" in remote_config:
            return await self.local_data_manager.sync_with_remote(
                Path(remote_config["remote_path"]),
                remote_config.get("direction", "both"),
            )
        return False

    async def rebuild_local_index(self) -> None:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰"""
        if self.local_data_manager:
            await self.local_data_manager.rebuild_index()

    async def get_local_data_stats(self) -> dict[str, Any]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’å–å¾—"""
        if self.local_data_manager:
            return await self.local_data_manager.get_local_stats()
        return {}

    async def auto_backup_if_needed(self) -> bool:
        """å¿…è¦ã«å¿œã˜ã¦è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        try:
            # æœ€å¾Œã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯
            backups = await self.list_backups()

            if not backups:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                await self.backup_vault("Auto backup - first backup")
                return True

            # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒ 24 æ™‚é–“ä»¥ä¸Šå‰ã®å ´åˆã¯æ–°ã—ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            from datetime import timedelta

            latest_backup = max(backups, key=lambda x: x.get("created_at", ""))
            latest_time = datetime.fromisoformat(latest_backup.get("created_at", ""))

            if datetime.now() - latest_time > timedelta(days=1):
                await self.backup_vault("Auto backup - daily")
                return True

            return False
        except Exception as e:
            self.logger.error("Failed to perform auto backup", error=str(e))
            return False

    def _matches_search_criteria(
        self,
        note,  # ObsidianNote or dict
        query: str | None = None,
        status=None,  # NoteStatus | None
        tags: list[str] | None = None,
        date_from=None,  # datetime | None
        date_to=None,  # datetime | None
    ) -> bool:
        """æ¤œç´¢æ¡ä»¶ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""

        # Note ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å¿…è¦ãªå±æ€§ã‚’å–å¾—
        if hasattr(note, "frontmatter"):
            frontmatter = note.frontmatter
            title = getattr(note, "title", "")
            content = getattr(note, "content", "")
            created_at = getattr(note, "created_at", None)
        elif isinstance(note, dict):
            frontmatter = note.get("frontmatter", {})
            title = note.get("title", "")
            content = note.get("content", "")
            created_at = note.get("created_at")
        else:
            return False

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
        if status and getattr(frontmatter, "status", None) != status:
            return False

        # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿
        if date_from and created_at and created_at < date_from:
            return False

        if date_to and created_at and created_at > date_to:
            return False

        # ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿
        if tags:
            note_tags = set()
            if hasattr(frontmatter, "tags") and frontmatter.tags:
                note_tags.update(frontmatter.tags)
            if hasattr(frontmatter, "ai_tags") and frontmatter.ai_tags:
                note_tags.update([tag.lstrip("#") for tag in frontmatter.ai_tags])

            if not any(tag in note_tags for tag in tags):
                return False

        # ã‚¯ã‚¨ãƒªãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚¿ã‚°ã€è¦ç´„ã‚’æ¤œç´¢ï¼‰
        if query:
            query_lower = query.lower()
            searchable_parts = [title.lower(), content.lower()]

            if hasattr(frontmatter, "tags") and frontmatter.tags:
                searchable_parts.append(" ".join(frontmatter.tags))
            if hasattr(frontmatter, "ai_tags") and frontmatter.ai_tags:
                searchable_parts.append(" ".join(frontmatter.ai_tags))
            if hasattr(frontmatter, "ai_summary") and frontmatter.ai_summary:
                searchable_parts.append(frontmatter.ai_summary)

            searchable_text = " ".join(searchable_parts)

            if query_lower not in searchable_text:
                return False

        return True
